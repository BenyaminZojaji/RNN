{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "123_g3AZuDe1cFgfoSFgo9l4d48o3rnFR",
      "authorship_tag": "ABX9TyOD9YLHCgrfz5pWLRMAv5zj"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XB3V37-ugnnT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Dropout, LSTM, GRU\n",
        "from keras.preprocessing import sequence"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read csv file\n",
        "def read_csv(file_name):\n",
        "  data_frame = pd.read_csv(file_name)\n",
        "  X = np.array(data_frame[\"sentence\"])\n",
        "  Y = np.array(data_frame[\"label\"], dtype=int) # labels are integer\n",
        "  return X, Y"
      ],
      "metadata": {
        "id": "p4BjP2j6hKuq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, Y_train = read_csv(\"/content/drive/MyDrive/Dataset/Emoji_Text_Classification/train.csv\")\n",
        "X_test, Y_test = read_csv(\"/content/drive/MyDrive/Dataset/Emoji_Text_Classification/test.csv\")"
      ],
      "metadata": {
        "id": "Gcad7RTlhlAe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get max length of sentences\n",
        "max_len = len(max(X_train, key=len).split(\" \"))\n",
        "max_len"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a709767iRYT",
        "outputId": "d061d6ba-cebb-4df5-8cba-4f448dbe6215"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace labels with related emoji\n",
        "def label_to_emoji(label):\n",
        "    emojies = [\"‚ù§Ô∏è\", \"üèê\", \"üòÑ\", \"üòû\", \"üç¥\"]\n",
        "    return emojies[label]\n",
        "\n",
        "index = 10\n",
        "print(X_train[index], label_to_emoji(Y_train[index]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2bLb-ITiU_m",
        "outputId": "73df14f4-035c-4f3b-adcf-6968ec78db83"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "she did not answer my text  üòû\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of sentence in each class\n",
        "unique, counts = np.unique(Y_train, return_counts=True)\n",
        "dict(zip(unique, counts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMaQCVSUiYpf",
        "outputId": "1ca57d41-08fc-4fb5-e064-fb2a8fa4d93d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 22, 1: 19, 2: 38, 3: 36, 4: 17}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Emojifier-V1"
      ],
      "metadata": {
        "id": "vkC69-h-iliz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert labels to one hot\n",
        "num_classes = len(np.unique(Y_train))\n",
        "\n",
        "Y_train_oh = tf.keras.utils.to_categorical(Y_train, num_classes)\n",
        "Y_test_oh = tf.keras.utils.to_categorical(Y_test, num_classes)"
      ],
      "metadata": {
        "id": "dP9DB0ISib1k"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = 5\n",
        "print(Y_train[index], \"is converted into one hot\", Y_train_oh[index])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qd4JpWH9it_Q",
        "outputId": "7b3ec9a9-ffec-4fee-db80-96eeeb56fbec"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 is converted into one hot [1. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download feature vectors and extract\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip -d glov.6B"
      ],
      "metadata": {
        "id": "Q6VgxBBKixTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read feature vectors and save them\n",
        "def read_glov_vectors(glove_file):\n",
        "  f = open(glove_file, encoding=\"utf8\")\n",
        "  words = set()\n",
        "  words_to_vec = dict()\n",
        "  for line in f:\n",
        "    line = line.strip().split()\n",
        "    word = line[0]\n",
        "    vec = line[1:]\n",
        "    words.add(word)\n",
        "    words_to_vec[word] = np.array(vec, dtype=np.float64)\n",
        "  return words_to_vec"
      ],
      "metadata": {
        "id": "spOcuL35i7wn"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_to_vec = read_glov_vectors(\"/content/glov.6B/glove.6B.50d.txt\")\n",
        "\n",
        "# Test the output of read_glov_vectors function\n",
        "words_to_vec[\"hello\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LN1oyyRMjA8f",
        "outputId": "f397c67e-55f5-4168-fa00-dc0fbad15017"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.38497 ,  0.80092 ,  0.064106, -0.28355 , -0.026759, -0.34532 ,\n",
              "       -0.64253 , -0.11729 , -0.33257 ,  0.55243 , -0.087813,  0.9035  ,\n",
              "        0.47102 ,  0.56657 ,  0.6985  , -0.35229 , -0.86542 ,  0.90573 ,\n",
              "        0.03576 , -0.071705, -0.12327 ,  0.54923 ,  0.47005 ,  0.35572 ,\n",
              "        1.2611  , -0.67581 , -0.94983 ,  0.68666 ,  0.3871  , -1.3492  ,\n",
              "        0.63512 ,  0.46416 , -0.48814 ,  0.83827 , -0.9246  , -0.33722 ,\n",
              "        0.53741 , -1.0616  , -0.081403, -0.67111 ,  0.30923 , -0.3923  ,\n",
              "       -0.55002 , -0.68827 ,  0.58049 , -0.11626 ,  0.013139, -0.57654 ,\n",
              "        0.048833,  0.67204 ])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Convert sentences to the average of the word vectors\n",
        "def sentence_to_avg(sentence):\n",
        "  words = sentence.lower().split() # Convert uppercase to lowercase\n",
        "  sum_vectors = np.zeros((50, ))\n",
        "  for w in words:\n",
        "    sum_vectors += words_to_vec[w]\n",
        "  avg_vectors = sum_vectors / len(words)\n",
        "  return avg_vectors"
      ],
      "metadata": {
        "id": "yk1hp7lPjFz3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test sentence_to_avg function\n",
        "sentence_to_avg(\"Pasta is my favorite food\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQary3yGjIoI",
        "outputId": "e78b9246-6f34-4d0e-9335-aab2086d4a16"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.242832  ,  0.370774  , -0.524396  ,  0.018644  ,  0.568756  ,\n",
              "        0.0219878 , -0.48206322, -0.152204  ,  0.235412  ,  0.1979466 ,\n",
              "       -0.178818  ,  0.3203976 ,  0.3379962 ,  0.1399654 ,  0.56775044,\n",
              "        0.118648  , -0.04531252,  0.335416  ,  0.149832  , -0.522814  ,\n",
              "        0.095746  , -0.0468764 ,  0.5508066 ,  0.39369132,  0.275182  ,\n",
              "       -1.275018  , -0.76076   ,  0.449102  ,  0.7542772 , -0.2332608 ,\n",
              "        2.82554   ,  0.287742  , -0.325976  ,  0.608572  , -0.020543  ,\n",
              "        0.286476  , -0.24984   ,  0.899408  ,  0.38995   , -0.270266  ,\n",
              "        0.3004734 ,  0.315962  , -0.2408782 ,  0.1586226 ,  0.5400462 ,\n",
              "        0.412066  , -0.1657008 , -0.253566  ,  0.3091806 ,  0.371192  ])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the average of all sentences\n",
        "X_train_avg = []\n",
        "for i in range(X_train.shape[0]):\n",
        "  X_train_avg.append(sentence_to_avg(X_train[i]))\n",
        "\n",
        "X_train_avg = np.array(X_train_avg)\n",
        "\n",
        "X_train_avg.shape, Y_train_oh.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjMH_puMjJ7A",
        "outputId": "4dd89b49-821b-4a1a-9600-31d0b395f94b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((132, 50), (132, 5))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model(using perceptron)\n",
        "class EmojiNet_V1(Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.dense = Dense(num_classes, input_shape=(50,), activation='softmax')\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.dense(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "mN5xfu6xjMCU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile and fit the model\n",
        "model = EmojiNet_V1()\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "              loss='categorical_crossentropy', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train_avg, Y_train_oh, epochs=400, shuffle=True)"
      ],
      "metadata": {
        "id": "Iqg5lblPjQtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "X_test_avg = []\n",
        "for i in range(X_test.shape[0]):\n",
        "    X_test_avg.append(sentence_to_avg(X_test[i]))\n",
        "\n",
        "X_test_avg = np.array(X_test_avg)\n",
        "model.evaluate(X_test_avg, Y_test_oh)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lvwjx_D8jTa4",
        "outputId": "2b53cc6e-b4ed-46a8-e665-8e8cfb2bed03"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 6ms/step - loss: 0.6285 - accuracy: 0.8393\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.628527820110321, 0.8392857313156128]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference\n",
        "X_me = np.array([\"not sad\", \"i adore you\", \"i love you\", \"funny lol\", \"lets play with a ball\", \"food is ready\", \"not feeling happy and funny\"])\n",
        "Y_me = np.array([[2], [0], [0], [2], [1], [4], [3]])\n",
        "X_me_avg = []\n",
        "\n",
        "for x in X_me:\n",
        "    X_me_avg.append(sentence_to_avg(x))\n",
        "\n",
        "X_me_avg = np.array(X_me_avg)\n",
        "pred = model.predict(X_me_avg)\n",
        "\n",
        "for i in range(X_me.shape[0]):\n",
        "    print(X_me[i], label_to_emoji(np.argmax(pred[i])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tp8nyi0jT7J",
        "outputId": "66113f14-af20-49d4-8d1c-4f7aa939c71f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 54ms/step\n",
            "not sad üòû\n",
            "i adore you ‚ù§Ô∏è\n",
            "i love you ‚ù§Ô∏è\n",
            "funny lol üòÑ\n",
            "lets play with a ball üèê\n",
            "food is ready üç¥\n",
            "not feeling happy and funny üòÑ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Emojifier-V2: Using RNNs"
      ],
      "metadata": {
        "id": "C3MhEqE_jcEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model\n",
        "class EmojiNet_V2(Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.lstm_1 = LSTM(2048, return_sequences=True)\n",
        "        self.dropout_1 = Dropout(0.3)\n",
        "        #self.lstm_2 = LSTM(512, return_sequences=True)\n",
        "        self.lstm_3 = LSTM(4096)\n",
        "        self.dropout_2 = Dropout(0.3)\n",
        "        self.dense = Dense(num_classes, activation='softmax')\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.lstm_1(x)\n",
        "        x = self.dropout_1(x)\n",
        "        # x = self.lstm_2(x)\n",
        "        # x = self.dropout_2(x)\n",
        "        x = self.lstm_3(x)\n",
        "        x = self.dropout_2(x)\n",
        "        x = self.dense(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "6hGhPcRfjZMm"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "model = EmojiNet_V2()\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001),\n",
        "              loss='categorical_crossentropy', \n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Juy_euoZjlBr"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix the size of all sentences to max_len\n",
        "def convert_sentences_to_embeddings(X):\n",
        "    emb_dim = words_to_vec[\"cucumber\"].shape[0]  # define dimensionality of your GloVe word vectors (= 50)\n",
        "    emb_matrix = np.zeros((X.shape[0], max_len, emb_dim))\n",
        "    for i in range(X.shape[0]):\n",
        "        words = X[i].lower().split()\n",
        "        for j in range(len(words)):\n",
        "            emb_matrix[i, j, :] = words_to_vec[words[j]]\n",
        "    return emb_matrix"
      ],
      "metadata": {
        "id": "E1kiVZYTjlmG"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test convert_sentences_to_embeddings function\n",
        "X_me = np.array([\"funny lol\", \"lets play baseball\", \"food is ready for you\"])\n",
        "print(X_me)\n",
        "print(convert_sentences_to_embeddings(X_me))"
      ],
      "metadata": {
        "id": "VLXFBAL_jsez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run convert_sentences_to_embeddings function for training data \n",
        "X_train_embs =convert_sentences_to_embeddings(X_train)\n",
        "X_train_embs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGFxgWyojwUO",
        "outputId": "8c9731fa-5e34-4ce9-edf9-0b0bad8928e1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(132, 10, 50)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train_embs, Y_train_oh, epochs=200, batch_size=4, shuffle=True)"
      ],
      "metadata": {
        "id": "y827gKhAjy86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "X_test_embs = convert_sentences_to_embeddings(X_test)\n",
        "print(X_test_embs.shape)\n",
        "model.evaluate(X_test_embs, Y_test_oh)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZofCixEjzUl",
        "outputId": "73ed38f4-9860-4045-ff43-b5e0d389418b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(56, 10, 50)\n",
            "2/2 [==============================] - 1s 46ms/step - loss: 1.1150 - accuracy: 0.8214\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.1150354146957397, 0.8214285969734192]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference\n",
        "X_me = np.array([\"not happy\", \"i adore you\", \"i love you\", \"funny lol\", \"lets play with a ball\", \"food is ready\", \"not feeling happy\", \"not feeling happy and funny\"])\n",
        "#Y_me = np.array([[2], [0], [0], [2], [1], [4], [3]])\n",
        "X_me_embed = convert_sentences_to_embeddings(X_me) \n",
        "\n",
        "pred = model.predict(X_me_embed)\n",
        "\n",
        "for i in range(X_me.shape[0]):\n",
        "    print(X_me[i], label_to_emoji(np.argmax(pred[i])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMh2dIInj5rL",
        "outputId": "a066b2e4-5880-4371-8c59-eb7eb1dcf505"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 685ms/step\n",
            "not happy üòû\n",
            "i adore you ‚ù§Ô∏è\n",
            "i love you ‚ù§Ô∏è\n",
            "funny lol üòÑ\n",
            "lets play with a ball üèê\n",
            "food is ready üç¥\n",
            "not feeling happy üòû\n",
            "not feeling happy and funny üòÑ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M9g0POHaj6eP"
      },
      "execution_count": 26,
      "outputs": []
    }
  ]
}